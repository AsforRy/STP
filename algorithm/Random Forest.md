# 🌲隨機森林(Random Forest)

是一種**集成學習（Ensemble Learning）的機器學習方法，屬於樹模型（Tree-based models）**的一種，主要應用於分類與回歸問題

---

## 🌲 原理

由 多棵決策樹（Decision Trees） 組成的模型，每棵樹會在訓練資料的不同子集上獨立訓練而成，最後再綜合每棵樹的預測結果進行投票（分類）或平均（回歸）

### 具體步驟：

1. 資料抽樣（Bootstrap sampling）：

- 對原始資料隨機抽樣出多個「有放回」的子集

- 每個子集用來訓練一棵決策樹

2. 特徵隨機性（Random Feature Selection）：

- 每棵樹在每個節點分裂時，隨機選擇部分特徵進行分裂判斷

- 這樣可以避免所有樹都長得一樣，增加多樣性

3. 投票或平均：

- 分類任務：每棵樹投票，最多票的類別為最終預測結果

- 回歸任務：取所有樹的預測平均值

## ✅ 優點（Advantages）

- 準確率高：比單棵決策樹通常更準確

- 不容易過擬合：多棵樹降低了單棵樹的過擬合風險

- 能處理大量資料與特徵

- 自帶特徵重要性分析（Feature Importance）

## ❌ 缺點（Disadvantages）

- 模型較大，不易解釋：不像單棵決策樹那樣容易理解

- 計算資源需求較高：訓練大量樹會花時間與記憶體

- 無法外插：對於特徵值範圍之外的新資料，預測能力有限（回歸任務特別明顯）

---

## 🧪 簡單例子

假設你要預測某人會不會購買產品（是/否）：

- 建立 100 棵決策樹

- 每棵樹用不同樣本與特徵訓練

- 測試時，100 棵樹各自給出預測（例如 70 棵說「是」，30 棵說「否」）

- 最終結果：多數決 = 是