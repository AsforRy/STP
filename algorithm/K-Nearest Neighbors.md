# 📘K-最近鄰(K-Nearest Neighbors)

KNN 並不是一種傳統意義上的「學習型」演算法，它屬於 懶惰學習（lazy learning）：

- 它不會主動建立模型

- 而是在預測時，直接根據資料集中的 K 筆最接近的資料來判斷類別或數值

---

## 🧠 什麼是 KNN？
KNN 是一種基於距離衡量的監督式學習演算法，主要用來解決分類與回歸問題（但多數應用於分類）。它的核心理念很簡單：

>「一筆新資料的類別，可以由它附近的 K 筆資料的類別來決定。」

這就像我們在生活中會根據鄰居的特徵來推測一個人的身分或喜好

## ⚙️ KNN 的核心原理（分類版）
1. 計算距離：
> 對於一筆要預測的資料點，計算它與訓練集中每一筆資料的距離  
常用的距離有：

- 歐氏距離（最常見）

- 曼哈頓距離

- 餘弦相似度（文字資料）

2. 找出 K 個最近的鄰居：
> 挑選距離最短的 K 筆資料，稱為「最近鄰居」

3. 投票決定類別：
> 這 K 筆鄰居中出現次數最多的類別，會被當作新資料的預測結果（多數決）

---

## 📝 舉例說明
假設你要預測某朵花的品種，你已知 100 朵花的數據與品種，現在來了一朵新花：

- 你計算這朵花與其他 100 朵的距離

- 找到距離最近的 5 朵（K=5）

- 發現這 5 朵花中有 3 朵是 Setosa，2 朵是 Virginica

- 所以你預測：這朵花是 Setosa

---

## ✅ 優點（優勢）

| 優點               | 說明                    |
| ---------------- | --------------------- |
| 💡 **簡單直觀**      | 原理清楚，易於理解與實作          |
| 🧪 **不需要訓練階段**   | 無需訓練模型，只需儲存資料         |
| 🧭 **自然處理非線性資料** | 不需要任何假設（不像邏輯回歸假設線性可分） |
| 📊 **隨時更新資料庫**   | 新資料加入後可以立即使用，不需重訓     |


## ⚠️ 缺點（限制）

| 缺點                   | 說明                        |
| -------------------- | ------------------------- |
| 🐢 **預測時間慢**         | 每次預測都需計算所有距離，資料量大時非常耗時    |
| ⚖️ **對特徵尺度敏感**       | 特徵單位不同會影響距離計算 → 需標準化或正規化  |
| 🎯 **對雜訊敏感**         | 如果鄰近的資料是錯誤標記的，可能造成預測錯誤    |
| 📉 **不擅長高維資料（維度災難）** | 維度過高時，每筆資料距離都差不多，無法判斷誰近誰遠 |
| 🧪 **K 值難選擇**        | 太小容易過擬合，太大會欠擬合，需用交叉驗證找最適值 |

---

## 🧮 補充說明：K 值怎麼選？
- 通常從小數（如 1, 3, 5, 7）開始試

- K 太小 → 過度依賴少數資料，可能過擬合

- K 太大 → 考慮太多不相關的資料，可能欠擬合

- 最佳方式：使用**交叉驗證（cross-validation）**來選出效果最好的 K