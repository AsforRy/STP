# 🌳 決策樹分類模型原理（Decision Tree Classifier）

## 🔍 決策樹是什麼？

決策樹是一種模擬「人類決策邏輯」的分類模型。它透過一連串「如果...那麼...」的判斷，將資料逐步切割，最終達到分類的目的。可以把整個分類過程視為一棵由上而下的樹，每個節點代表一個判斷條件，最後的葉節點則是分類結果

---

## 📈 原理概念（如何運作）

1. 從整體資料出發：模型會先觀察所有特徵與標籤之間的關聯

2. 選擇最佳的分割特徵與條件：

- 決策樹會試圖找出一個「特徵」及其「閾值」，用來將資料分為兩群，使這兩群之間的「純度」提升

- 例如：以 花瓣長度 > 2.5cm 作為分割條件，看看這樣能不能有效區分不同品種的花

3. 重複切割資料：

- 每次分割後，資料會被送往新的子節點，再次重複「找最佳分割特徵」的流程，直到：

    - 分類結果足夠純（幾乎全都是同一類）

    - 到達預設的最大深度

    - 資料筆數太少，無法再有效分割

4. 形成一棵分類樹：

- 最後形成一棵樹狀結構，每條從根到葉的路徑都是一套判斷邏輯

## 📏 常用的「最佳分割依據」

為了判斷資料切割是否有效，決策樹會使用下列指標：

| 指標                              | 說明                             |
| ------------------------------- | ------------------------------ |
| **Gini Impurity（基尼不純度）**        | 衡量分類的不純度，數值越小表示越純（例如一群幾乎都是同一類） |
| **Entropy 熵（Information Gain）** | 衡量資訊混亂程度，選擇能讓不確定性下降最多的分割方式     |

---

## ✅ 優點總結

| 優點             | 說明                      |
| -------------- | ----------------------- |
| ✅ **易於理解與視覺化** | 決策邏輯清楚，可畫成樹狀圖，易於向非專業者解釋 |
| ✅ **不需特別處理特徵** | 不需標準化、正規化，可同時處理類別與數值資料  |
| ✅ **可處理非線性關係** | 不需假設資料的線性關係，適合複雜資料      |
| ✅ **快速預測**     | 預測過程只需依序走樹的路徑，速度快       |

## ❌ 缺點總結

| 缺點                        | 說明                              |
| ------------------------- | ------------------------------- |
| ❌ **容易過度擬合（Overfitting）** | 若不加限制，樹可能太深，對訓練資料記得太細，導致測試資料表現差 |
| ❌ **對資料變動敏感**             | 資料稍有改變，整棵樹的結構可能會完全不同（不穩定）       |
| ❌ **無法預測機率平滑過渡**          | 分類邊界為直角狀，不像邏輯回歸有平滑機率變化          |
| ❌ **計算資源會隨資料量與特徵數上升**     | 若特徵多且需找最佳切點，計算成本可能高             |
