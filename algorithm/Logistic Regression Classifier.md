# 📘邏輯回歸（Logistic Regression）

---

## 1️⃣ 模型核心概念

邏輯回歸是一種機率型的分類模型，它的目標是預測某一個樣本屬於某個類別的機率。

儘管名字裡有「回歸」，但它實際上是用來做分類任務，特別是：

- 二元分類（如：是否為某疾病）

- 或延伸成多元分類（如：Iris 分類中的三種花）

## 2️⃣ 數學基礎

邏輯回歸的本質是使用一條「線性函數」來預測資料點的結果，但會透過一個Sigmoid 函數來將這個線性預測值轉換成機率：

👉 預測公式：

```python
z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b
```

這裡的 z 是線性組合（和線性回歸一樣），w 是每個特徵的權重，x 是特徵數值，b 是偏差項

👉 Sigmoid 函數轉換：

```python
P(y=1|x) = 1 / (1 + e^(-z))
```

這個公式會把 z 的值壓縮到 0 到 1 之間，代表某類別的機率

👉 分類決策：

- 若機率 > 0.5，預測為類別 1

- 若機率 < 0.5，預測為類別 0

- 多類別情況下會使用 softmax 函數（邏輯回歸的延伸版本）來分配每個類別的機率

## 3️⃣ 模型訓練方式

邏輯回歸的目標是找到一組參數（權重）w 和 b，讓模型對訓練資料的預測越準越好

- 透過最大化**對數似然函數（Log Likelihood）**來進行參數學習

- 最常見的優化方法是使用梯度下降法（Gradient Descent）

---

## ✅ 優點（Advantages）

1. 簡單直觀，易於實作與解釋
> 每個特徵對結果的貢獻可以用權重來說明，是一種「可解釋性高」的模型

2. 計算效率高
>> 適合中小型資料集，可以快速訓練與預測

3. 可輸出機率
>> 相對於只給出分類結果，邏輯回歸可以告訴你「預測為某類的機率」

4. 不需要太多參數調整
>> 相對於複雜模型，邏輯回歸的超參數較少，容易訓練與調整

5. 適用於線性可分的資料
>> 如果不同類別的資料點之間可以用一條直線（或超平面）區分，效果會非常好

## ❌ 缺點（Disadvantages）

1. 只能擬合線性邊界
>> 若資料之間是非線性分界（例如彎曲形狀），邏輯回歸可能表現很差

2. 對於離群值敏感
>> 極端數值可能會大幅影響模型的預測

3. 需進行特徵縮放與轉換
>> 如果特徵尺度差異太大，模型會較難學習正確的權重，需搭配標準化

4. 對特徵間共線性敏感
>> 若某些特徵彼此高度相關，會影響模型穩定性與解釋性

5. 效能限制
>> 在高維度或複雜資料（如影像、語音）中表現不佳，容易被更複雜的模型（如決策樹、SVM、深度學習）取代