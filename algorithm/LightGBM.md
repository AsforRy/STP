# Light Gradient Boosting Machine (LightGBM)

一種 梯度提升（Gradient Boosting）框架，是 GBDT 的高效版本，專門設計來加速訓練速度並處理大規模資料集，同時保有高準確度

它是集成學習（Ensemble Learning）的一種實作，透過多棵決策樹的集成，將弱學習器（例如淺層樹）疊加成強學習器，並不斷修正前一棵樹的預測誤差

---

## 原理

1. Leaf-wise Tree Growth（葉子優先生長策略）

    與傳統 GBDT 採用 Level-wise（層層擴展） 不同，LightGBM 採用 Leaf-wise（葉子優先） 的建樹方式，每次從所有葉子節點中挑選損失下降最多的葉子節點進行擴展

    - 優點：更容易捕捉複雜的資料分布 → 準確率通常更高

    - 缺點：若不限制深度，容易造成過擬合

2. Histogram-based Decision Tree

    LightGBM 不直接處理原始連續數值，而是先將特徵值離散化成直方圖 bin，大幅減少了運算量與記憶體消耗，並提升速度。

3. 支援類別型特徵

    與許多模型需手動轉換類別變數不同，LightGBM 可直接處理類別型資料，自動選擇最佳分割方式

## 優點

| 類別       | 優勢內容                            |
| -------- | ------------------------------- |
| ⏱️ 速度    | 建樹速度非常快，尤其在大數據下表現優異             |
| 🧠 準確性   | Leaf-wise 策略使其常有比傳統 GBDT 更高的準確率 |
| 🧱 資源效率  | 使用 histogram 算法降低記憶體與 CPU 需求    |
| 🔍 類別處理  | 可直接接受類別變數，無需 one-hot encoding   |
| ❌ 缺值容忍   | 可自動處理缺失值（missing value）         |
| 📊 多任務支援 | 同時支援分類、回歸、排序等任務                 |

## 缺點

| 缺點項目           | 說明                                       |
| -------------- | ---------------------------------------- |
| 🧮 超參數多        | 需調整多個參數（如 num\_leaves, max\_depth）以避免過擬合 |
| 📉 小資料集未必有明顯優勢 | 在小型資料集上，效能與傳統模型差異不大                      |
| 🤯 理解成本        | 相對於邏輯回歸或 KNN，模型原理與調參較複雜                  |
| 📐 過擬合風險       | Leaf-wise 策略若無限制容易導致過度擬合                 |

## 應用場景

- 財務風險預測（如信用評分、詐欺檢測）

- 行為預測（如廣告點擊率預測）

- 醫療診斷（疾病分類）

- 自然語言處理中分類任務（如情感分析）

---

## 範圍

現在有一份資料包含：

- 使用者年齡

- 瀏覽時間（分鐘）

- 是否是會員

- 當前活動折扣百分比

- 最後是否購買（是或否）

希望用這些資料建立一個模型來預測新用戶是否會購買商品

### 原始資料（模擬）

| 年齡 | 瀏覽時間 | 會員 | 折扣  | 是否購買 |
| -- | ---- | -- | --- | ---- |
| 25 | 10   | 否  | 5%  | 否    |
| 40 | 45   | 是  | 10% | 是    |
| 30 | 20   | 是  | 0%  | 否    |
| 22 | 60   | 否  | 15% | 是    |

### 做法

1. 把資料轉換成數字（例如：會員→1, 否→0）

2. 建立 LightGBM 模型並訓練它

3. 給它一個新的用戶資料，例如：

    - 年齡：35、瀏覽時間：30分鐘、是會員、折扣：5%

4. 模型預測結果：「有 80% 機率會購買」

### 為什麼選 LightGBM？

- 效率高：幾千萬筆資料也能快速訓練

- 準確率好：比起傳統邏輯回歸可能預測得更準

- 自動處理「是否是會員」這種類別變數

- 可直接處理缺失值（例如有些人沒填年齡）
