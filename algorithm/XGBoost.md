# Extreme Gradient Boosting (XGBoost)

是一種強大的 集成式學習演算法，屬於 梯度提升（Gradient Boosting），是一種專為效率與準確率優化的提升樹（Boosting Tree）模型，廣泛應用於分類、回歸與排序等任務

---

## 原理

基於 Boosting 的思想：將多個弱分類器（如決策樹）串聯起來，每個模型專注於改正前一個模型犯下的錯誤，逐步提高整體模型的準確性

其主要流程如下：

1. 從一個簡單模型開始（如一棵淺層決策樹）

2. 計算該模型的預測誤差（即殘差）

3. 訓練下一棵樹專門去預測這些殘差

4. 逐步重複這個過程，每次都「修正誤差」，直到模型收斂或達到預設迭代次數

## 特點

| 特點         | 說明                          |
| ---------- | --------------------------- |
| 🎯 梯度提升    | 每棵新樹都是基於損失函數的梯度來生成的         |
| 🛡️ 正則化    | 避免模型過度擬合（overfitting）       |
| ⚡ 高效訓練     | 支援多執行緒與特徵剪枝（pruning），大幅提升速度 |
| 🔧 自動處理缺失值 | 不需額外填補遺漏資料                  |
| 📊 特徵重要性分析 | 可輸出每個特徵對預測結果的重要性排序          |
| 📈 支援早停法   | 可設定條件提早終止訓練，防止浪費資源          |

## 與傳統 Gradient Boosting 差異

| 比較項目  | 傳統 Gradient Boosting | XGBoost           |
| ----- | -------------------- | ----------------- |
| 正則化功能 | 無                    | 有（L1, L2）         |
| 並行處理  | 不支援                  | 支援（大幅加速）          |
| 缺失值處理 | 需手動處理                | 自動處理              |
| 損失函數  | 限制較多                 | 使用二階泰勒展開，可自定義損失函數 |

## 優點

- 極高準確率，常在實戰中表現優異

- 訓練速度快，支援 CPU/GPU 加速

- 自動處理缺失值與不平衡資料

- 可進行特徵選擇與分析

## 缺點

- 模型較難解釋（如同大多數 ensemble 模型）

- 超參數多，需花時間調整

- 對於資料量小或特徵單純的問題，未必優於簡單模型（如 Logistic Regression）

## 應用範圍

- 金融風險評估（如信用評分）

- 醫療診斷（疾病分類）

- 行銷預測（顧客流失、購買行為）

- 搜尋排序（如搜尋引擎中的 Rank 系統）

- 比賽與專案（例如 Titanic 生存預測、房價預測、客戶分群）

---

## 範例

有一位老師要幫學生改作文，評分標準包括：

- 文法正確

- 內容通順

- 結構完整

但老師不是一次就給出最終分數，而是分成多個階段，每次「找出上一輪沒改好的錯誤」，就像 XGBoost 一樣

### 第一步（第一棵樹）：

老師先粗略看作文，根據「文法錯誤」給了一個分數（這就是模型的初始預測）

→ 但分數還不準，有些作文其實內容很好卻被扣太多分

### 第二步（第二棵樹）：

老師再檢查：「我第一輪是根據文法錯誤給分，但有些內容通順的應該加分。」

→ 第二輪開始修正第一輪的錯誤（這就是學習殘差）

### 第三步（第三棵樹）：

接著再針對「結構不完整」的問題進行微調分數

→ 每一輪都在修正上一輪評分的誤差

### 最後一步：

綜合所有評分，老師給出了一個最終分數，比一開始只用文法給分更合理

## 結論

- 每一棵樹都在修正前一棵樹的預測錯誤

- 最終模型是所有決策樹「預測結果的加總」

- 模型越學越準，直到誤差足夠小或達到指定次數

XGBoost 就像是「一群會修正錯誤的老師」，每一位老師（決策樹）都把上位老師做得不好的地方補上，最後給出一個很精準的評分（預測結果）