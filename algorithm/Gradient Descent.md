# 梯度下降法（Gradient Descent）

是一種最佳化演算法，主要用來找出某個函數的最小值，在機器學習中常用來訓練模型、調整參數，讓模型預測誤差最小化

---

## 🧠 基本概念
梯度下降就像「在山上摸黑往下走」，每次往斜率最陡的下坡方向移動一小步，直到抵達谷底（最小值）為止

### 📌 數學公式（單變數為例）：

假設我們的損失函數是：

> L(w) = 誤差值（例如平方誤差、對數損失）

更新參數的公式為：

> w_new = w_old - η × ∇L(w)

其中：

- ∇L(w) 是損失函數對參數 w 的偏導數（梯度）

- η 是學習率（learning rate），控制每次走多遠

## 🔁 迭代流程（多變數的話，每個參數都這樣更新）

1. 初始化一組參數（隨機給定 w 和 b）

2. 計算損失函數的梯度（也就是「斜率」）

3. 沿著梯度的相反方向更新參數

4. 重複步驟 2~3，直到收斂（損失變化非常小）

## ⚖️ 學習率（Learning Rate）的影響

- 太小：學得慢，需很多次迭代才會收斂

- 太大：可能會錯過最佳解，甚至導致震盪不穩

## 📚 類型

| 類型                                    | 說明                    |
| ------------------------------------- | --------------------- |
| **Batch Gradient Descent**            | 每次使用整筆資料計算梯度，準確但慢     |
| **Stochastic Gradient Descent (SGD)** | 每次只用一筆資料計算，速度快但不穩定    |
| **Mini-batch Gradient Descent**       | 每次用一小批資料，兼顧速度與穩定性，最常用 |

## 🔍 為什麼需要梯度下降法？

以邏輯回歸為例：

- 我們的目標是找到一組參數（權重 w 和偏差 b），讓預測值越接近真實值越好。

- 我們會設計一個「損失函數」（Loss Function）來量化預測錯誤。

- 這個損失函數通常是一個多變數的曲面，而梯度下降法就是在這個曲面上找到最低點的方法

## ✅ 優點

- 適用於大量參數與資料的最佳化問題

- 可搭配多種優化技巧（如動量法、Adam）

## ❌ 缺點

- 需選定學習率

- 可能陷入局部最小值或鞍點（尤其在非凸函數中）

---

## 🌱 梯度下降簡單例子（用一元函數）

### 🔢 問題：

找出函數

> f(x) = (x - 3)² 的最小值

你可以看出這是一個拋物線，最小值就是 x = 3，因為：

> f(x) = (x - 3)² → 當 x = 3 時，f(x) = 0（最小）

## 🧮 我們如何用梯度下降找到 x = 3？

### 1️⃣ 先隨便猜一個起點，比如 x = 0

（你可以想像你站在山坡上的某一點）

### 2️⃣ 計算梯度（也就是導數）

> f'(x) = 2(x - 3)

在 x = 0 時：

> f'(0) = 2(0 - 3) = -6 表示這裡斜率是負的（往右下斜），要「往右走」

### 3️⃣ 選一個學習率 η，例如 η = 0.1

然後更新 x：

> x_new = x_old - η × f'(x)  
x = 0 - 0.1 × (-6) = 0 + 0.6 = 0.6

### 4️⃣ 再來一次

現在 x = 0.6

> f'(0.6) = 2(0.6 - 3) = -4.8  
x = 0.6 + 0.48 = 1.08

繼續重複，你會看到：

| 步驟 | x 值   | 導數 f'(x) | 更新後 x 值 |
| -- | ----- | -------- | ------- |
| 0  | 0.00  | -6       | 0.60    |
| 1  | 0.60  | -4.8     | 1.08    |
| 2  | 1.08  | -3.84    | 1.46    |
| 3  | 1.46  | -3.08    | 1.77    |
| …  | …     | …        | …       |
| 最終 | 約 3.0 | 約 0      | 停止      |

會發現 x 慢慢逼近 3，而 f(x) 也越來越小

## 🎯 小結

這個例子展示了：

- 如何從一個初始值出發

- 根據函數的斜率（導數）決定走的方向

- 一步一步調整 x，直到接近最小值