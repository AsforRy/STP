# 梯度下降法（Gradient Descent）

是一種最佳化演算法，主要用來找出某個函數的最小值，在機器學習中常用來訓練模型、調整參數，讓模型預測誤差最小化

---

## 🧠 基本概念
梯度下降就像「在山上摸黑往下走」，每次往斜率最陡的下坡方向移動一小步，直到抵達谷底（最小值）為止

### 📌 數學公式（單變數為例）：

假設我們的損失函數是：

> L(w) = 誤差值（例如平方誤差、對數損失）

更新參數的公式為：

> w_new = w_old - η × ∇L(w)

其中：

- ∇L(w) 是損失函數對參數 w 的偏導數（梯度）

- η 是學習率（learning rate），控制每次走多遠

## 🔁 迭代流程（多變數的話，每個參數都這樣更新）

1. 初始化一組參數（隨機給定 w 和 b）

2. 計算損失函數的梯度（也就是「斜率」）

3. 沿著梯度的相反方向更新參數

4. 重複步驟 2~3，直到收斂（損失變化非常小）

## ⚖️ 學習率（Learning Rate）的影響

- 太小：學得慢，需很多次迭代才會收斂

- 太大：可能會錯過最佳解，甚至導致震盪不穩

## 📚 類型

| 類型                                    | 說明                    |
| ------------------------------------- | --------------------- |
| **Batch Gradient Descent**            | 每次使用整筆資料計算梯度，準確但慢     |
| **Stochastic Gradient Descent (SGD)** | 每次只用一筆資料計算，速度快但不穩定    |
| **Mini-batch Gradient Descent**       | 每次用一小批資料，兼顧速度與穩定性，最常用 |

## 🔍 為什麼需要梯度下降法？

以邏輯回歸為例：

- 我們的目標是找到一組參數（權重 w 和偏差 b），讓預測值越接近真實值越好。

- 我們會設計一個「損失函數」（Loss Function）來量化預測錯誤。

- 這個損失函數通常是一個多變數的曲面，而梯度下降法就是在這個曲面上找到最低點的方法

## ✅ 優點

- 適用於大量參數與資料的最佳化問題

- 可搭配多種優化技巧（如動量法、Adam）

## ❌ 缺點

- 需選定學習率

- 可能陷入局部最小值或鞍點（尤其在非凸函數中）