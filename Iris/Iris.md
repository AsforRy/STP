
# ğŸŒ¸ Iris è³‡æ–™åˆ†æèˆ‡åˆ†é¡æ¨¡å‹ç¯„ä¾‹

æœ¬ç¯„ä¾‹ä»¥ç¶“å…¸çš„ Iris èŠ±å‰è³‡æ–™é›†ï¼Œå¯¦ä½œè³‡æ–™å‰è™•ç†ã€æ¢ç´¢æ€§è³‡æ–™åˆ†æã€æ¨¡å‹è¨“ç·´èˆ‡æ··æ·†çŸ©é™£è©•ä¼°ã€‚

---

## 1ï¸âƒ£ è³‡æ–™è¼‰å…¥èˆ‡åˆæ­¥æª¢æŸ¥

```python
df = pd.read_csv('Iris.csv')
df = df.drop(columns=["Id"])
df.head()
df.describe()
df.info()
```

---

## 2ï¸âƒ£ è³‡æ–™é è™•ç†

```python
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['Species'] = le.fit_transform(df['Species'])
df.isnull().sum()
```

- LabelEncoder å°‡èŠ±ç¨®è½‰æˆæ•¸å­— 0, 1, 2
- æª¢æŸ¥ç¼ºå¤±å€¼ï¼šçš†ç‚º 0ï¼Œè³‡æ–™å®Œæ•´

---

## 3ï¸âƒ£ æ¢ç´¢æ€§è³‡æ–™åˆ†æï¼ˆEDAï¼‰

### ğŸ”¢ æ•¸å€¼åˆ†å¸ƒï¼ˆç›´æ–¹åœ–ï¼‰

```python
df['SepalLengthCm'].hist()
df['SepalWidthCm'].hist()
df['PetalLengthCm'].hist()
df['PetalWidthCm'].hist()
```

### ğŸŒˆ ç‰¹å¾µé—œä¿‚åœ–ï¼ˆæ•£ä½ˆåœ–ï¼‰

```python
plt.scatter(df['SepalLengthCm'], df['SepalWidthCm'], c=colors)
```

- å¤šç¨®æ•£ä½ˆåœ–å±•ç¤ºèŠ±è¼ã€èŠ±ç“£çš„å¹¾ä½•ç‰¹å¾µ

---

## 4ï¸âƒ£ ç‰¹å¾µç›¸é—œæ€§åˆ†æ

```python
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
```

- é«˜ç›¸é—œï¼šPetalLengthCm vs PetalWidthCm

---

## 5ï¸âƒ£ æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´

```python
X = df.drop(columns=['Species'])
Y = df['Species']
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)
```

### ğŸ¤– æ¨¡å‹å»ºæ§‹èˆ‡è¨“ç·´

```python
# Logistic Regression
model1 = LogisticRegression()
model1.fit(x_train, y_train)

# K-Nearest Neighbors
model2 = KNeighborsClassifier()
model2.fit(x_train, y_train)

# Decision Tree
model3 = DecisionTreeClassifier()
model3.fit(x_train, y_train)
```

---

## 6ï¸âƒ£ æ¨¡å‹è©•ä¼°ï¼šæº–ç¢ºç‡

```python
print(model1.score(x_test, y_test))
print(model2.score(x_test, y_test))
print(model3.score(x_test, y_test))
```

---

## 7ï¸âƒ£ æ··æ·†çŸ©é™£ï¼ˆConfusion Matrixï¼‰

```python
confusion_matrix(y_test, model.predict(x_test))
sns.heatmap(...)
```

- è¦–è¦ºåŒ–ä¸‰å€‹æ¨¡å‹çš„é æ¸¬æº–ç¢ºæ€§
- åˆ†æåˆ†é¡éŒ¯èª¤é¡å‹èˆ‡æ•¸é‡

---

## âœ… æ¨¡å‹æ¯”è¼ƒç¸½çµï¼ˆä¸€èˆ¬æƒ…æ³ä¸‹çš„è¡¨ç¾ï¼‰

| æ¨¡å‹                      | å„ªé»                      | ç¼ºé»                 |
| ----------------------- | ----------------------- | ------------------ |
| **Logistic Regression** | è¨ˆç®—å¿«ï¼Œé©åˆç·šæ€§å¯åˆ†å•é¡Œï¼›å¯è§£é‡‹æ€§å¼·      | é‡åˆ°éç·šæ€§é‚Šç•Œæ•ˆæœå·®         |
| **K-Nearest Neighbors** | ç°¡å–®ç›´è§€ã€éåƒæ•¸å¼æ–¹æ³•ï¼›èƒ½è™•ç†éç·šæ€§é—œä¿‚    | å¤§è³‡æ–™æ…¢ã€å°é›œè¨Šæ•æ„Ÿã€ä¸é©åˆé«˜ç¶­è³‡æ–™ |
| **Decision Tree**       | å¯è™•ç†éç·šæ€§ã€å¯è¦–åŒ–ã€èƒ½è™•ç†é¡åˆ¥èˆ‡æ•¸å€¼æ··åˆè³‡æ–™ | å®¹æ˜“éæ“¬åˆï¼Œå°¤å…¶æ˜¯æ²’æœ‰é™åˆ¶æ·±åº¦æ™‚   |

## ğŸ§  ç¸½çµå»ºè­°

| å¦‚æœ...                | é¸ç”¨é€™å€‹æ¨¡å‹              |
| -------------------- | ------------------- |
| ä½ è¦å¿«é€Ÿå»ºç«‹åŸºæº–æ¨¡å‹ï¼ˆbaselineï¼‰ | Logistic Regression |
| ä½ æƒ³æ•æ‰éç·šæ€§é—œä¿‚ã€è³‡æ–™é‡ä¸å¤§      | KNN                 |
| ä½ è¦å¯è¦–åŒ–åˆ†é¡é‚è¼¯ã€è™•ç†è¤‡é›œé‚Šç•Œ     | Decision Tree       |

## ğŸ§  æœ€ä½³é æ¸¬æ¨¡å‹ï¼šé€šå¸¸æ˜¯ K-Nearest Neighbors (KNN)

- åŸå› ï¼š
- Iris è³‡æ–™é›†çš„ä¸‰å€‹èŠ±ç¨®ï¼ˆSetosa, Versicolor, Virginicaï¼‰ï¼š
- åœ¨ç‰¹å¾µç©ºé–“ä¸­ è‡ªç„¶åˆ†ç¾¤æ˜é¡¯ï¼Œé‚Šç•Œéç·šæ€§ã€‚
- ç‰¹åˆ¥æ˜¯ Setosa å¹¾ä¹èˆ‡å…¶ä»–å…©ç¨®å®Œå…¨åˆ†é›¢ï¼ŒKNN å®¹æ˜“æŒæ¡é€™ç¨®åˆ†ä½ˆã€‚

- KNN æ˜¯éåƒæ•¸æ¨¡å‹ï¼š
- å®ƒä¸å‡è¨­ç‰¹å¾µèˆ‡é¡åˆ¥ä¹‹é–“çš„ç·šæ€§é—œä¿‚ï¼Œåè€Œèƒ½å½ˆæ€§é©æ‡‰æ•¸æ“šåˆ†ä½ˆã€‚
- ç›¸æ¯” Logistic Regressionï¼ˆåªæ“…é•·ç·šæ€§é‚Šç•Œï¼‰æ•ˆæœé€šå¸¸æ›´å¥½ã€‚

- Decision Tree ä¹Ÿæœƒæœ‰ä¸éŒ¯è¡¨ç¾ï¼š
- ä½†è‹¥æ²’æœ‰åšå‰ªæï¼ˆå¦‚é™åˆ¶æ·±åº¦ã€min_samples_leafï¼‰ï¼Œå®¹æ˜“éæ“¬åˆï¼Œå°¤å…¶æ˜¯åœ¨è¨“ç·´æº–ç¢ºç‡é«˜ã€æ¸¬è©¦ä¸‹é™çš„æƒ…æ³ä¸‹ã€‚

## ğŸ“Š æ¨¡å‹é æ¸¬è¡¨ç¾ï¼ˆä¸€èˆ¬ç¶“é©—å€¼ç¯„åœï¼‰

| æ¨¡å‹                  | é æ¸¬æº–ç¢ºç‡ï¼ˆä¼°è¨ˆï¼‰       |
| ------------------- | --------------- |
| Logistic Regression | ç´„ 93â€“96%        |
| **KNN**             | âœ… é€šå¸¸ 96â€“100%    |
| Decision Tree       | ç´„ 93â€“98%ï¼Œæœ‰æ™‚æœƒéæ“¬åˆ |
